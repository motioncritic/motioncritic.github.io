<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <!-- <title>Academic Project Page</title> -->
  <title>Aligning Human Motion Generation with Human Perceptions</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	    tex2jax: {
	        inlineMath: [['$','$'], ['\\(','\\)']],
	        processEscapes: true
	    }
	});
    </script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="xtitle is-1 publication-title">Aligning Human Motion Generation with Human Perceptions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ou524u.github.io/" target="_blank">Haoru Wang<sup>1,†</sup></a>
              </span>
              <span class="author-block">
                <a href="https://walter0807.github.io/" target="_blank">Wentao Zhu<sup>1,†</sup></a>
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Luyi Miao<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="FORTH AUTHOR PERSONAL LINK" target="_blank">Yishu Xu<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="THIRD LAST AUTHOR PERSONAL LINK" target="_blank">Feng Gao<sup>1</sup></a>
              </span>
              <span class="author-block">
                <a href="https://www.qitian1987.com/index.html" target="_blank">Qi Tian<sup>2</sup></a>
              </span>
              <span class="author-block">
                <a href="https://cfcs.pku.edu.cn/wangyizhou/" target="_blank">Yizhou Wang<sup>1</sup></a>
              </span>
              <div class="is-size-5 publication-authors">
		  <span class="author-block">
		    <sup>1</sup> Peking University, <sup>2</sup> Huawei Cloud, <sup>†</sup> Lead author
		  </span>
		  <div class="is-size-4" style="margin-top: 5px;">
		    ICLR 2025
		  </div>
		</div>
<!--               <div class="is-size-6">
                <p><sup>†</sup> Lead author</p>
              </div> -->

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2407.02272" target="_blank" class="external-link ">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link ">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ou524u/MotionCritic" target="_blank" class="external-link ">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2407.02272" target="_blank" class="external-link ">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>


  <!-- New section for images -->
  <div class="image-wrapper section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Overview</h2>
          <div class="image-item">
            <img src="static/images/framework.png" alt="Framework" class="responsive-image">
            <p class="image-caption">We collect MotionPercept, a large-scale, human-annotated
              dataset for
              motion perceptual evaluation, where human subjects select the best quality motion in multiple-choice
              questions.
              Using this dataset, we train MotionCritic to automatically judge motion quality in alignment with human
              perceptions,
              offering better quality metrics. Additionally, we show that MotionCritic can enhance existing motion
              generators
              with
              minimal fine-tuning.</p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Paper abstract -->
  <!-- <section class="section hero is-light"> -->
  <section class="section hero is-white">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Abstract</h2>
          <div class="content has-text-justified">
            <p>Human motion generation is a critical task with a wide spectrum of applications. Achieving high realism
              in
              generated motions requires naturalness, smoothness, and plausibility. However, current evaluation metrics
              often rely on error with ground-truth, simple heuristics, or distribution distances and do not align well
              with human perceptions. In this work, we propose a data-driven approach to bridge this gap by introducing
              a large-scale human perceptual evaluation dataset, MotionPercept, and a human motion critic model,
              MotionCritic, that
              capture human perceptual preferences. Our critic model offers a more accurate metric for assessing motion
              quality and could be readily integrated into the motion generation pipeline to enhance generation quality.
              Extensive experiments demonstrate the effectiveness of our approach in both evaluating and improving the
              quality of generated human motions by aligning with human perceptions.</p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- MotionPercept section -->
  <div class="image-wrapper section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">MotionPercept</h2>
          <div class="image-item">
            <p class="image-caption">We first introduce a large-scale human perceptual evaluation dataset,
              MotionPercept.</p>
          </div>
          <div class="image-item">
            <video src="static/videos/00-1.mp4" autoplay loop muted class="responsive-image"></video>
            <p class="image-caption">
              We implement a pipeline for data collection and annotation. First, we collect
              generated human motion sequence pairs, and then instruct the annotators to select the best candidate. The
              best option should be the most natural, visually pleasing, and free of artifacts.</p>
          </div>
        </div>
      </div>
    </div>
  </div>


  <!-- MotionCritic Section -->
  <div class="image-wrapper section hero is-white">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">MotionCritic</h2>
          <div class="image-item">
            <img src="static/images/pipeline.png" alt="Pipeline" class="responsive-image">
            <p class="image-caption">(I) Critic model training process. We sample human motion pairs $\mathbf{x}^{(h)},
              \mathbf{x}^{(l)}$ annotated with human preferences, upon which the critic model produces score pairs. We
              use perceptual alignment loss $L_\text{Percept}$ to learn from the human perceptions.
              <br>
              (II) Motion generation with critic model supervision. We intercept MDM sampling process at random timestep
              $t$ and perform single-step prediction. Critic model computes the score $s$ based on the generated motion
              $\mathbf{x}_0'$, which is further used to calculate motion critic loss $L_\text{Critic}$.

              KL loss $L_\text{KL}$ is introduced between $\mathbf{x}_0'$ and last-time generation result
              $\widetilde{\mathbf{x}_0}'$.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>



  <div class="image-wrapper section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">MotionCritic as Motion Quality Metric</h2>
          <div class="columns is-multiline">
            <!-- First video and caption -->
            <div class="column is-half">
              <div class="video-item">
                <video src="static/videos/04-1.mp4" autoplay loop muted class="responsive-video"></video>
              </div>
            </div>
            <!-- Second video and caption -->
            <div class="column is-half">
              <div class="video-item">
                <video src="static/videos/05-1.mp4" autoplay loop muted class="responsive-video"></video>
              </div>
            </div>
            <p class="image-caption">The MotionCritic model scores motion based on human preference alignment and can
              serve as a motion quality metric. Here, we present the results on the test set, demonstrating that the
              critic score effectively reflects motion quality.</p>

            <p class="image-caption">
              We further test the generalization of our critic model on the GT motion distribution. We group the motions
              in the HumanAct12 test set into five evenly distributed groups based on their critic scores, labeled GT-I
              to GT-V from highest to lowest. The distribution of critic scores for GT motions is shown in the
              histogram.</p>
            <img src="static/images/histogram.png" alt="Framework" class="responsive-image">

            <p class="image-caption">
              We discover that the outliers with small critic values are indeed artifacts within the dataset, as shown
              in the video. This demonstrates that MotionCritic can be used for dataset diagnosis.</p>
            <video src="static/videos/06-1.mp4" autoplay loop muted class="responsive-image"></video>

            <p class="image-caption">
              We also compare the motions across the groups and find that critic scores align well with motion quality,
              as confirmed by extensive user studies.</p>
            <video src="static/videos/07-1.mp4" autoplay loop muted class="responsive-image"></video>
          </div>
        </div>
      </div>
    </div>
  </div>



  <div class="image-wrapper section hero is-white">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">MotionCritic as Training Supervision</h2>
          <div class="image-item">
            <p class="image-caption">We show that fine-tuning with our critic model significantly improves motion
              quality with higher critic scores. Notably, this requires only a few hundred iterations, demonstrating the
              effectiveness and efficiency of our method.</p>

          </div>
          <div class="image-item">
            <video src="static/videos/01-1.mp4" autoplay loop muted class="responsive-image"></video>
            <video src="static/videos/02-1.mp4" autoplay loop muted class="responsive-image"></video>
            <video src="static/videos/03-1.mp4" autoplay loop muted class="responsive-image"></video>
          </div>
        </div>
      </div>
    </div>
  </div>



  <style>
    .image-wrapper,
    .section.hero.is-light {
      padding: 3rem 1.5rem;
      background-color: #f5f5f5;
    }

    .section.hero.is-white {
      padding: 3rem 1.5rem;
      background-color: #ffffff;
    }

    .container.is-max-desktop {
      max-width: 960px;
      margin: auto;
    }

    .columns.is-centered {
      display: flex;
      justify-content: center;
    }

    .column.is-four-fifths {
      width: 90%;
    }

    .image-item {
      text-align: left;
    }

    .responsive-image {
      max-width: 100%;
      height: auto;
    }

    .image-caption {
      margin-top: 10px;
      font-size: 1.2em;
      color: #000000;
      padding: 10px;
      background-color: rgba(1, 1, 1, 0);
      text-align: left;
    }

    .content {
      font-size: 1.2em;
    }

    .title.is-3 {
      font-size: 2em;
      text-align: center;
    }

    .has-text-justified {
      text-align: justify;
    }

    .has-text-centered {
      text-align: center;
    }

    video::-webkit-media-controls {
      display: none !important;
    }

    video {
      -webkit-appearance: none;
      -moz-appearance: none;
      appearance: none;
    }
  </style>




  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/sfFFWTpQcEQ" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{motionpercept2025,
    title={Aligning Motion Generation with Human Perceptions},
    author={Wang, Haoru and Zhu, Wentao and Miao, Luyi and Xu, Yishu and Gao, Feng and Tian, Qi and Wang, Yizhou},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/pdf/2407.02272}
}</code></pre>
    </div>
  </section>
<section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgement</h2>
        <p>If you use MotionPercept and MotionCritic in your work, please also cite the original datasets and methods on which our work is based.</p>

        <h3>MDM:</h3>
        <pre><code>@inproceedings{
    tevet2023human,
    title={Human Motion Diffusion Model},
    author={Guy Tevet and Sigal Raab and Brian Gordon and Yoni Shafir and Daniel Cohen-or and Amit Haim Bermano},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=SJ1kSyO2jwu}
}</code></pre>

        <h3>HumanAct12:</h3>
        <pre><code>@inproceedings{guo2020action2motion,
    title={Action2motion: Conditioned generation of 3d human motions},
    author={Guo, Chuan and Zuo, Xinxin and Wang, Sen and Zou, Shihao and Sun, Qingyao and Deng, Annan and Gong, Minglun and Cheng, Li},
    booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
    pages={2021--2029},
    year={2020}
}</code></pre>

        <h3>FLAME:</h3>
        <pre><code>@inproceedings{kim2023flame,
    title={Flame: Free-form language-based motion synthesis \& editing},
    author={Kim, Jihoon and Kim, Jiseob and Choi, Sungjoon},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={37},
    number={7},
    pages={8255--8263},
    year={2023}
}</code></pre>

        <h3>UESTC:</h3>
        <pre><code>@inproceedings{ji2018large,
    title={A large-scale RGB-D database for arbitrary-view human action recognition},
    author={Ji, Yanli and Xu, Feixiang and Yang, Yang and Shen, Fumin and Shen, Heng Tao and Zheng, Wei-Shi},
    booktitle={Proceedings of the 26th ACM international Conference on Multimedia},
    pages={1510--1518},
    year={2018}
}</code></pre>

        <h3>DSTFormer:</h3>
        <pre><code>@inproceedings{zhu2023motionbert,
    title={Motionbert: A unified perspective on learning human motion representations},
    author={Zhu, Wentao and Ma, Xiaoxuan and Liu, Zhaoyang and Liu, Libin and Wu, Wayne and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={15085--15099},
    year={2023}
}</code></pre>

        <h3>SMPL:</h3>
        <pre><code>@incollection{loper2023smpl,
    title={SMPL: A skinned multi-person linear model},
    author={Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J},
    booktitle={Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
    pages={851--866},
    year={2023}
}</code></pre>

        <p>We also recommend exploring other motion metrics, including 
            <a href="https://arxiv.org/abs/2207.13807">PoseNDF</a>, 
            <a href="https://arxiv.org/abs/1809.03036">NPSS</a>, 
            <a href="http://gall.cv-uni-bonn.de/download/jgall_forecastintention_3dv21.pdf">NDMS</a>, 
            <a href="https://arxiv.org/abs/2309.10248">MoBERT</a>, and 
            <a href="https://arxiv.org/abs/2211.10658">PFC</a>. 
            You can also check out a 
            <a href="https://arxiv.org/abs/2307.10894">survey</a> of different motion generation metrics.
        </p>
    </div>
</section>

  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/vinthony/project-page-template">modification
                version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> from <a
                href="https://github.com/vinthony">vinthony</a>.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>




</body>

</html>
